{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.ma as ma\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.preprocessing as prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model import AugementedConvLSTM\n",
    "import configparser\n",
    "import argparse\n",
    "import h5py\n",
    "import itertools\n",
    "\n",
    "projection_dimensions = [129,135]\n",
    "end_index = 7305\n",
    "# import pandas as pd\n",
    "# len(pd.date_range(start='1/1/1948', end='12/31/1967'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 03:47:05.409674: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-28 03:47:08.601585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8688 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:18:00.0, compute capability: 7.5\n",
      "2022-06-28 03:47:08.605508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9631 MB memory:  -> device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:3b:00.0, compute capability: 7.5\n",
      "2022-06-28 03:47:08.608996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9631 MB memory:  -> device: 2, name: GeForce RTX 2080 Ti, pci bus id: 0000:86:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "Aug_ConvLSTM_model = AugementedConvLSTM(\n",
    "    projection_height = 129, \n",
    "    projection_width = 135,\n",
    "    timesteps=4\n",
    "    )\n",
    "model = Aug_ConvLSTM_model.model(\n",
    "    [32, 16, 16], \n",
    "    [9,5,3], \n",
    "    [64,32,1], \n",
    "    [9,3,5], \n",
    "    2\n",
    "    )\n",
    "model.load_weights('convlstm_weights_pr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(mask = [1,1,1,1,1], rf_idx = 1, end_index = 21185):\n",
    "    filepath = r\"/home/uditbhatia/Documents/Sarth/Downscaling_AugmentedConvLSTM/05_Random_Field_India/Data\"\n",
    "    rf = np.array(np.load(os.path.join(filepath,rf\"r{rf_idx}.npy\")))[:end_index,:,:]\n",
    "    X = np.empty((7,end_index,129,135))\n",
    "    X[0,:,:,:] = np.array(np.load(r\"MIROC-ESM.npy\"))[:end_index,:,:] if mask[0] else np.array(rf)\n",
    "    X[1,:,:,:] = np.array(np.load(r\"elev.npy\"))[:end_index,:,:] if mask[1] else np.array(rf)\n",
    "    X[2,:,:,:] = np.array(np.load(os.path.join(filepath,rf\"rhum.npy\")))[:end_index,:,:] if mask[2] else np.array(rf)\n",
    "    X[3,:,:,:] = np.array(np.load(os.path.join(filepath,rf\"pres.npy\")))[:end_index,:,:] if mask[3] else np.array(rf)\n",
    "    X[4,:,:,:] = np.array(np.load(os.path.join(filepath,rf\"uwnd.npy\")))[:end_index,:,:] if mask[4] else np.array(rf)\n",
    "    X[5,:,:,:] = np.array(np.load(os.path.join(filepath,rf\"vwnd.npy\")))[:end_index,:,:] if mask[4] else np.array(rf)\n",
    "    X[6,:,:,:] = np.array(np.load(os.path.join(filepath,rf\"omega.npy\")))[:end_index,:,:] if mask[4] else np.array(rf)\n",
    "    # print(X.shape)\n",
    "    Y = np.load(rf\"/home/uditbhatia/Documents/Sarth/Downscaling_AugmentedConvLSTM/05_Random_Field_India/Data/IMD.npy\")[:,:end_index,:,:]\n",
    "    return X, Y\n",
    "\n",
    "def normalize(data):\n",
    "    data = data - data.mean()\n",
    "    data = data / data.std()\n",
    "    return data\n",
    "\n",
    "def set_data(X, Y, end_index = 21185):\n",
    "    X_normalized = np.zeros((7, end_index, 129, 135))\n",
    "    for i in range(7):\n",
    "        X_normalized[i,] = normalize(X[i,])\n",
    "    Y_normalized = normalize(Y)\n",
    "    std_observed = Y.std()\n",
    "    X = X_normalized.transpose(1,2,3,0)\n",
    "    Y = Y_normalized.reshape(-1,129, 135, 1)\n",
    "    return X, Y, std_observed\n",
    "\n",
    "def data_generator(X,Y): # 25+5 ... 15+5 = 1962\n",
    "    min_train_year = 1948\n",
    "    max_train_year = 1962 #1973 #1999\n",
    "    min_test_year = 1963 #1974 #2000\n",
    "    max_test_year = 1967 #1979 #2005\n",
    "    # 1948-1973-1974-1979\n",
    "    total_years = max_test_year - min_train_year + 1\n",
    "    train_years = max_train_year - min_train_year + 1\n",
    "    n_days = np.max(X.shape)\n",
    "    train_days = int((n_days/total_years)*train_years)\n",
    "    train_x, train_y = X[:train_days], Y[:train_days]\n",
    "    test_x, test_y = X[train_days:], Y[train_days:]\n",
    "    time_steps = 4\n",
    "    batch_size1 = 15\n",
    "    train_generator = prep.sequence.TimeseriesGenerator(\n",
    "        train_x, \n",
    "        train_y.reshape(-1, projection_dimensions[0], projection_dimensions[1], 1),\n",
    "        length=time_steps, \n",
    "        batch_size=batch_size1\n",
    "        )\n",
    "    test_generator = prep.sequence.TimeseriesGenerator(\n",
    "        test_x, \n",
    "        test_y.reshape(-1, projection_dimensions[0], projection_dimensions[1], 1),\n",
    "        length=time_steps, \n",
    "        batch_size=batch_size1\n",
    "        )\n",
    "    return train_generator, test_generator\n",
    "\n",
    "def train(clstm_model, train_generator, test_generator, load_weights = False, std_observed = 1.0):\n",
    "\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "    def actual_rmse_loss(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square((y_pred - y_true)*std_observed)))\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "    \n",
    "    clstm_model.compile(optimizer=adam, loss=root_mean_squared_error, metrics=[root_mean_squared_error, actual_rmse_loss])\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f\"convlstm_weights_pr_modified.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=f\"./Graphs/norm_csltm_pre_Graph\", histogram_freq=0, write_graph=True, write_images=False)\n",
    "    termnan = tf.keras.callbacks.TerminateOnNaN()\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_delta=0.005, min_lr=0.000004, verbose=1)\n",
    "    \n",
    "    callbacks_list = [checkpoint,tensorboard, reduce_lr, termnan]\n",
    "    \n",
    "    history = clstm_model.fit(\n",
    "        train_generator, \n",
    "        callbacks=callbacks_list, \n",
    "        epochs=4, \n",
    "        validation_data=test_generator,\n",
    "        verbose=1\n",
    "        )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_list = [ # Pr, elev, rh, p, wnd\n",
    "    [1,1,1,1,1], #0\n",
    "    [1,0,1,1,1], #1\n",
    "    [1,1,0,1,1], #2\n",
    "    [1,1,1,0,1], #3\n",
    "    [1,1,1,1,0], #4\n",
    "    [0,1,1,1,0], #5\n",
    "    [0,1,1,0,1], #6\n",
    "    [0,1,0,1,1], #7\n",
    "    [0,1,1,0,0], ##8\n",
    "    [0,1,0,0,1], ##9\n",
    "    [0,1,0,1,0], ##10\n",
    "    [0,1,0,0,0], ###11\n",
    "    [0,0,1,0,0], ###12\n",
    "    [0,0,0,1,0], ###13\n",
    "    [0,0,0,0,1], ###14\n",
    "    [1,0,0,0,0],\n",
    "    [0,0,0,0,0]\n",
    "]\n",
    "# np.random.seed(0)\n",
    "# np.random.shuffle(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uditbhatia/anaconda3/envs/Sarth-tf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "2022-06-28 03:48:00.181232: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 7605\n",
      "2022-06-28 03:48:01.114909: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Running ptxas --version returned 32512\n",
      "2022-06-28 03:48:01.605293: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: ptxas exited with non-zero error code 32512, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - ETA: 0s - loss: 0.7954 - root_mean_squared_error: 0.7953 - actual_rmse_loss: 4.5868\n",
      "Epoch 00001: val_loss improved from inf to 0.75340, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 269s 710ms/step - loss: 0.7954 - root_mean_squared_error: 0.7953 - actual_rmse_loss: 4.5868 - val_loss: 0.7534 - val_root_mean_squared_error: 0.7524 - val_actual_rmse_loss: 4.3393 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7681 - root_mean_squared_error: 0.7680 - actual_rmse_loss: 4.4293\n",
      "Epoch 00002: val_loss improved from 0.75340 to 0.74437, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 262s 717ms/step - loss: 0.7681 - root_mean_squared_error: 0.7680 - actual_rmse_loss: 4.4293 - val_loss: 0.7444 - val_root_mean_squared_error: 0.7433 - val_actual_rmse_loss: 4.2873 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7561 - root_mean_squared_error: 0.7560 - actual_rmse_loss: 4.3604\n",
      "Epoch 00003: val_loss improved from 0.74437 to 0.73096, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 261s 714ms/step - loss: 0.7561 - root_mean_squared_error: 0.7560 - actual_rmse_loss: 4.3604 - val_loss: 0.7310 - val_root_mean_squared_error: 0.7300 - val_actual_rmse_loss: 4.2101 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7466 - root_mean_squared_error: 0.7465 - actual_rmse_loss: 4.3056\n",
      "Epoch 00004: val_loss did not improve from 0.73096\n",
      "365/365 [==============================] - 260s 713ms/step - loss: 0.7466 - root_mean_squared_error: 0.7465 - actual_rmse_loss: 4.3056 - val_loss: 0.7325 - val_root_mean_squared_error: 0.7315 - val_actual_rmse_loss: 4.2189 - lr: 3.0000e-04\n",
      "[1, 1, 0, 1, 1]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7516 - root_mean_squared_error: 0.7515 - actual_rmse_loss: 4.3342\n",
      "Epoch 00001: val_loss improved from inf to 0.73644, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 266s 714ms/step - loss: 0.7516 - root_mean_squared_error: 0.7515 - actual_rmse_loss: 4.3342 - val_loss: 0.7364 - val_root_mean_squared_error: 0.7355 - val_actual_rmse_loss: 4.2422 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7281 - root_mean_squared_error: 0.7280 - actual_rmse_loss: 4.1990\n",
      "Epoch 00002: val_loss improved from 0.73644 to 0.72333, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 261s 716ms/step - loss: 0.7281 - root_mean_squared_error: 0.7280 - actual_rmse_loss: 4.1990 - val_loss: 0.7233 - val_root_mean_squared_error: 0.7223 - val_actual_rmse_loss: 4.1662 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7167 - root_mean_squared_error: 0.7166 - actual_rmse_loss: 4.1330\n",
      "Epoch 00003: val_loss improved from 0.72333 to 0.71128, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 261s 714ms/step - loss: 0.7167 - root_mean_squared_error: 0.7166 - actual_rmse_loss: 4.1330 - val_loss: 0.7113 - val_root_mean_squared_error: 0.7103 - val_actual_rmse_loss: 4.0969 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7106 - root_mean_squared_error: 0.7105 - actual_rmse_loss: 4.0978\n",
      "Epoch 00004: val_loss improved from 0.71128 to 0.71047, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 261s 714ms/step - loss: 0.7106 - root_mean_squared_error: 0.7105 - actual_rmse_loss: 4.0978 - val_loss: 0.7105 - val_root_mean_squared_error: 0.7095 - val_actual_rmse_loss: 4.0922 - lr: 3.0000e-04\n",
      "[1, 1, 1, 0, 1]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7499 - root_mean_squared_error: 0.7498 - actual_rmse_loss: 4.3244\n",
      "Epoch 00001: val_loss improved from inf to 0.71012, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 266s 715ms/step - loss: 0.7499 - root_mean_squared_error: 0.7498 - actual_rmse_loss: 4.3244 - val_loss: 0.7101 - val_root_mean_squared_error: 0.7092 - val_actual_rmse_loss: 4.0905 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7246 - root_mean_squared_error: 0.7245 - actual_rmse_loss: 4.1787\n",
      "Epoch 00002: val_loss improved from 0.71012 to 0.69147, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 262s 719ms/step - loss: 0.7246 - root_mean_squared_error: 0.7245 - actual_rmse_loss: 4.1787 - val_loss: 0.6915 - val_root_mean_squared_error: 0.6907 - val_actual_rmse_loss: 3.9834 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7142 - root_mean_squared_error: 0.7141 - actual_rmse_loss: 4.1188\n",
      "Epoch 00003: val_loss improved from 0.69147 to 0.68746, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 262s 717ms/step - loss: 0.7142 - root_mean_squared_error: 0.7141 - actual_rmse_loss: 4.1188 - val_loss: 0.6875 - val_root_mean_squared_error: 0.6867 - val_actual_rmse_loss: 3.9605 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7079 - root_mean_squared_error: 0.7079 - actual_rmse_loss: 4.0826\n",
      "Epoch 00004: val_loss did not improve from 0.68746\n",
      "365/365 [==============================] - 261s 715ms/step - loss: 0.7079 - root_mean_squared_error: 0.7079 - actual_rmse_loss: 4.0826 - val_loss: 0.6895 - val_root_mean_squared_error: 0.6886 - val_actual_rmse_loss: 3.9716 - lr: 3.0000e-04\n",
      "[1, 1, 1, 1, 0]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7748 - root_mean_squared_error: 0.7747 - actual_rmse_loss: 4.4681\n",
      "Epoch 00001: val_loss improved from inf to 0.72827, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 267s 717ms/step - loss: 0.7748 - root_mean_squared_error: 0.7747 - actual_rmse_loss: 4.4681 - val_loss: 0.7283 - val_root_mean_squared_error: 0.7273 - val_actual_rmse_loss: 4.1950 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7589 - root_mean_squared_error: 0.7588 - actual_rmse_loss: 4.3765\n",
      "Epoch 00002: val_loss improved from 0.72827 to 0.72614, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 263s 719ms/step - loss: 0.7589 - root_mean_squared_error: 0.7588 - actual_rmse_loss: 4.3765 - val_loss: 0.7261 - val_root_mean_squared_error: 0.7252 - val_actual_rmse_loss: 4.1825 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7515 - root_mean_squared_error: 0.7514 - actual_rmse_loss: 4.3337\n",
      "Epoch 00003: val_loss improved from 0.72614 to 0.72262, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 262s 717ms/step - loss: 0.7515 - root_mean_squared_error: 0.7514 - actual_rmse_loss: 4.3337 - val_loss: 0.7226 - val_root_mean_squared_error: 0.7217 - val_actual_rmse_loss: 4.1624 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7473 - root_mean_squared_error: 0.7472 - actual_rmse_loss: 4.3096\n",
      "Epoch 00004: val_loss improved from 0.72262 to 0.71852, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 262s 717ms/step - loss: 0.7473 - root_mean_squared_error: 0.7472 - actual_rmse_loss: 4.3096 - val_loss: 0.7185 - val_root_mean_squared_error: 0.7176 - val_actual_rmse_loss: 4.1386 - lr: 3.0000e-04\n"
     ]
    }
   ],
   "source": [
    "for mask in itertools.islice(mask_list, 1, 5, 1):\n",
    "    print(mask)\n",
    "    X, Y = load_dataset(mask = mask, end_index = end_index)\n",
    "    X, Y, std_observed = set_data(X,Y,end_index=end_index)\n",
    "    print(std_observed)\n",
    "    train_generator, test_generator = data_generator(X, Y)\n",
    "    history = train(\n",
    "        model,\n",
    "        train_generator, \n",
    "        test_generator,\n",
    "        load_weights = True,\n",
    "        std_observed = std_observed\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 0]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7510 - root_mean_squared_error: 0.7509 - actual_rmse_loss: 4.3310\n",
      "Epoch 00001: val_loss improved from inf to 0.71590, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 268s 719ms/step - loss: 0.7510 - root_mean_squared_error: 0.7509 - actual_rmse_loss: 4.3310 - val_loss: 0.7159 - val_root_mean_squared_error: 0.7150 - val_actual_rmse_loss: 4.1237 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7454 - root_mean_squared_error: 0.7453 - actual_rmse_loss: 4.2985\n",
      "Epoch 00002: val_loss improved from 0.71590 to 0.71442, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 263s 720ms/step - loss: 0.7454 - root_mean_squared_error: 0.7453 - actual_rmse_loss: 4.2985 - val_loss: 0.7144 - val_root_mean_squared_error: 0.7135 - val_actual_rmse_loss: 4.1150 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7428 - root_mean_squared_error: 0.7427 - actual_rmse_loss: 4.2838\n",
      "Epoch 00003: val_loss did not improve from 0.71442\n",
      "365/365 [==============================] - 262s 718ms/step - loss: 0.7428 - root_mean_squared_error: 0.7427 - actual_rmse_loss: 4.2838 - val_loss: 0.7185 - val_root_mean_squared_error: 0.7176 - val_actual_rmse_loss: 4.1385 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7404 - root_mean_squared_error: 0.7403 - actual_rmse_loss: 4.2699\n",
      "Epoch 00004: val_loss improved from 0.71442 to 0.71169, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 262s 719ms/step - loss: 0.7404 - root_mean_squared_error: 0.7403 - actual_rmse_loss: 4.2699 - val_loss: 0.7117 - val_root_mean_squared_error: 0.7108 - val_actual_rmse_loss: 4.0996 - lr: 3.0000e-04\n",
      "[0, 1, 1, 0, 1]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7256 - root_mean_squared_error: 0.7255 - actual_rmse_loss: 4.1846\n",
      "Epoch 00001: val_loss improved from inf to 0.68691, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 268s 720ms/step - loss: 0.7256 - root_mean_squared_error: 0.7255 - actual_rmse_loss: 4.1846 - val_loss: 0.6869 - val_root_mean_squared_error: 0.6860 - val_actual_rmse_loss: 3.9565 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7090 - root_mean_squared_error: 0.7089 - actual_rmse_loss: 4.0885\n",
      "Epoch 00002: val_loss improved from 0.68691 to 0.68632, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 263s 721ms/step - loss: 0.7090 - root_mean_squared_error: 0.7089 - actual_rmse_loss: 4.0885 - val_loss: 0.6863 - val_root_mean_squared_error: 0.6854 - val_actual_rmse_loss: 3.9531 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7019 - root_mean_squared_error: 0.7018 - actual_rmse_loss: 4.0478\n",
      "Epoch 00003: val_loss did not improve from 0.68632\n",
      "365/365 [==============================] - 262s 718ms/step - loss: 0.7019 - root_mean_squared_error: 0.7018 - actual_rmse_loss: 4.0478 - val_loss: 0.6983 - val_root_mean_squared_error: 0.6975 - val_actual_rmse_loss: 4.0229 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.6984 - root_mean_squared_error: 0.6983 - actual_rmse_loss: 4.0276\n",
      "Epoch 00004: val_loss improved from 0.68632 to 0.68159, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 262s 718ms/step - loss: 0.6984 - root_mean_squared_error: 0.6983 - actual_rmse_loss: 4.0276 - val_loss: 0.6816 - val_root_mean_squared_error: 0.6809 - val_actual_rmse_loss: 3.9270 - lr: 3.0000e-04\n",
      "[0, 1, 0, 1, 1]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7303 - root_mean_squared_error: 0.7302 - actual_rmse_loss: 4.2116\n",
      "Epoch 00001: val_loss improved from inf to 0.73006, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 267s 717ms/step - loss: 0.7303 - root_mean_squared_error: 0.7302 - actual_rmse_loss: 4.2116 - val_loss: 0.7301 - val_root_mean_squared_error: 0.7291 - val_actual_rmse_loss: 4.2050 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7145 - root_mean_squared_error: 0.7144 - actual_rmse_loss: 4.1206\n",
      "Epoch 00002: val_loss improved from 0.73006 to 0.71988, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 263s 719ms/step - loss: 0.7145 - root_mean_squared_error: 0.7144 - actual_rmse_loss: 4.1206 - val_loss: 0.7199 - val_root_mean_squared_error: 0.7189 - val_actual_rmse_loss: 4.1464 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7075 - root_mean_squared_error: 0.7074 - actual_rmse_loss: 4.0801\n",
      "Epoch 00003: val_loss did not improve from 0.71988\n",
      "365/365 [==============================] - 262s 716ms/step - loss: 0.7075 - root_mean_squared_error: 0.7074 - actual_rmse_loss: 4.0801 - val_loss: 0.7204 - val_root_mean_squared_error: 0.7195 - val_actual_rmse_loss: 4.1497 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7035 - root_mean_squared_error: 0.7034 - actual_rmse_loss: 4.0568\n",
      "Epoch 00004: val_loss did not improve from 0.71988\n",
      "365/365 [==============================] - 262s 717ms/step - loss: 0.7035 - root_mean_squared_error: 0.7034 - actual_rmse_loss: 4.0568 - val_loss: 0.7267 - val_root_mean_squared_error: 0.7258 - val_actual_rmse_loss: 4.1860 - lr: 3.0000e-04\n",
      "[0, 1, 1, 0, 0]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7741 - root_mean_squared_error: 0.7740 - actual_rmse_loss: 4.4638\n",
      "Epoch 00001: val_loss improved from inf to 0.73203, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 269s 721ms/step - loss: 0.7741 - root_mean_squared_error: 0.7740 - actual_rmse_loss: 4.4638 - val_loss: 0.7320 - val_root_mean_squared_error: 0.7310 - val_actual_rmse_loss: 4.2163 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7571 - root_mean_squared_error: 0.7569 - actual_rmse_loss: 4.3657\n",
      "Epoch 00002: val_loss improved from 0.73203 to 0.72652, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 264s 722ms/step - loss: 0.7571 - root_mean_squared_error: 0.7569 - actual_rmse_loss: 4.3657 - val_loss: 0.7265 - val_root_mean_squared_error: 0.7255 - val_actual_rmse_loss: 4.1846 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7512 - root_mean_squared_error: 0.7511 - actual_rmse_loss: 4.3321\n",
      "Epoch 00003: val_loss improved from 0.72652 to 0.72076, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 263s 720ms/step - loss: 0.7512 - root_mean_squared_error: 0.7511 - actual_rmse_loss: 4.3321 - val_loss: 0.7208 - val_root_mean_squared_error: 0.7198 - val_actual_rmse_loss: 4.1513 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7496 - root_mean_squared_error: 0.7495 - actual_rmse_loss: 4.3229\n",
      "Epoch 00004: val_loss did not improve from 0.72076\n",
      "365/365 [==============================] - 263s 721ms/step - loss: 0.7496 - root_mean_squared_error: 0.7495 - actual_rmse_loss: 4.3229 - val_loss: 0.7269 - val_root_mean_squared_error: 0.7260 - val_actual_rmse_loss: 4.1870 - lr: 3.0000e-04\n",
      "[0, 1, 0, 0, 1]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7290 - root_mean_squared_error: 0.7289 - actual_rmse_loss: 4.2038\n",
      "Epoch 00001: val_loss improved from inf to 0.73925, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 269s 721ms/step - loss: 0.7290 - root_mean_squared_error: 0.7289 - actual_rmse_loss: 4.2038 - val_loss: 0.7393 - val_root_mean_squared_error: 0.7384 - val_actual_rmse_loss: 4.2590 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7125 - root_mean_squared_error: 0.7124 - actual_rmse_loss: 4.1089\n",
      "Epoch 00002: val_loss improved from 0.73925 to 0.72936, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 264s 723ms/step - loss: 0.7125 - root_mean_squared_error: 0.7124 - actual_rmse_loss: 4.1089 - val_loss: 0.7294 - val_root_mean_squared_error: 0.7286 - val_actual_rmse_loss: 4.2022 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7064 - root_mean_squared_error: 0.7063 - actual_rmse_loss: 4.0736\n",
      "Epoch 00003: val_loss did not improve from 0.72936\n",
      "365/365 [==============================] - 263s 721ms/step - loss: 0.7064 - root_mean_squared_error: 0.7063 - actual_rmse_loss: 4.0736 - val_loss: 0.7310 - val_root_mean_squared_error: 0.7302 - val_actual_rmse_loss: 4.2114 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7038 - root_mean_squared_error: 0.7037 - actual_rmse_loss: 4.0589\n",
      "Epoch 00004: val_loss improved from 0.72936 to 0.71674, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 262s 718ms/step - loss: 0.7038 - root_mean_squared_error: 0.7037 - actual_rmse_loss: 4.0589 - val_loss: 0.7167 - val_root_mean_squared_error: 0.7160 - val_actual_rmse_loss: 4.1294 - lr: 3.0000e-04\n",
      "[0, 1, 0, 1, 0]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8189 - root_mean_squared_error: 0.8188 - actual_rmse_loss: 4.7224\n",
      "Epoch 00001: val_loss improved from inf to 0.77012, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 268s 721ms/step - loss: 0.8189 - root_mean_squared_error: 0.8188 - actual_rmse_loss: 4.7224 - val_loss: 0.7701 - val_root_mean_squared_error: 0.7691 - val_actual_rmse_loss: 4.4359 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8158 - root_mean_squared_error: 0.8157 - actual_rmse_loss: 4.7047\n",
      "Epoch 00002: val_loss improved from 0.77012 to 0.76866, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 263s 721ms/step - loss: 0.8158 - root_mean_squared_error: 0.8157 - actual_rmse_loss: 4.7047 - val_loss: 0.7687 - val_root_mean_squared_error: 0.7677 - val_actual_rmse_loss: 4.4275 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8142 - root_mean_squared_error: 0.8141 - actual_rmse_loss: 4.6953\n",
      "Epoch 00003: val_loss improved from 0.76866 to 0.76849, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 262s 718ms/step - loss: 0.8142 - root_mean_squared_error: 0.8141 - actual_rmse_loss: 4.6953 - val_loss: 0.7685 - val_root_mean_squared_error: 0.7675 - val_actual_rmse_loss: 4.4263 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8130 - root_mean_squared_error: 0.8129 - actual_rmse_loss: 4.6884\n",
      "Epoch 00004: val_loss improved from 0.76849 to 0.76794, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 263s 719ms/step - loss: 0.8130 - root_mean_squared_error: 0.8129 - actual_rmse_loss: 4.6884 - val_loss: 0.7679 - val_root_mean_squared_error: 0.7669 - val_actual_rmse_loss: 4.4232 - lr: 3.0000e-04\n"
     ]
    }
   ],
   "source": [
    "for mask in itertools.islice(mask_list, 5, 11, 1):\n",
    "    print(mask)\n",
    "    X, Y = load_dataset(mask = mask, end_index = end_index)\n",
    "    X, Y, std_observed = set_data(X,Y,end_index=end_index)\n",
    "    print(std_observed)\n",
    "    train_generator, test_generator = data_generator(X, Y)\n",
    "    history = train(\n",
    "        model,\n",
    "        train_generator, \n",
    "        test_generator,\n",
    "        load_weights = True,\n",
    "        std_observed = std_observed\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8174 - root_mean_squared_error: 0.8173 - actual_rmse_loss: 4.7140\n",
      "Epoch 00001: val_loss improved from inf to 0.76869, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 268s 721ms/step - loss: 0.8174 - root_mean_squared_error: 0.8173 - actual_rmse_loss: 4.7140 - val_loss: 0.7687 - val_root_mean_squared_error: 0.7677 - val_actual_rmse_loss: 4.4275 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8155 - root_mean_squared_error: 0.8154 - actual_rmse_loss: 4.7029\n",
      "Epoch 00002: val_loss did not improve from 0.76869\n",
      "365/365 [==============================] - 264s 722ms/step - loss: 0.8155 - root_mean_squared_error: 0.8154 - actual_rmse_loss: 4.7029 - val_loss: 0.7719 - val_root_mean_squared_error: 0.7709 - val_actual_rmse_loss: 4.4461 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8153 - root_mean_squared_error: 0.8152 - actual_rmse_loss: 4.7015\n",
      "Epoch 00003: val_loss did not improve from 0.76869\n",
      "365/365 [==============================] - 263s 721ms/step - loss: 0.8153 - root_mean_squared_error: 0.8152 - actual_rmse_loss: 4.7015 - val_loss: 0.7689 - val_root_mean_squared_error: 0.7678 - val_actual_rmse_loss: 4.4284 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8146 - root_mean_squared_error: 0.8145 - actual_rmse_loss: 4.6974\n",
      "Epoch 00004: val_loss did not improve from 0.76869\n",
      "365/365 [==============================] - 263s 720ms/step - loss: 0.8146 - root_mean_squared_error: 0.8145 - actual_rmse_loss: 4.6974 - val_loss: 0.7688 - val_root_mean_squared_error: 0.7678 - val_actual_rmse_loss: 4.4282 - lr: 3.0000e-04\n",
      "[0, 0, 1, 0, 0]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8169 - root_mean_squared_error: 0.8168 - actual_rmse_loss: 4.7109\n",
      "Epoch 00001: val_loss improved from inf to 0.76435, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 270s 724ms/step - loss: 0.8169 - root_mean_squared_error: 0.8168 - actual_rmse_loss: 4.7109 - val_loss: 0.7644 - val_root_mean_squared_error: 0.7633 - val_actual_rmse_loss: 4.4026 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8044 - root_mean_squared_error: 0.8043 - actual_rmse_loss: 4.6387\n",
      "Epoch 00002: val_loss improved from 0.76435 to 0.76313, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 265s 725ms/step - loss: 0.8044 - root_mean_squared_error: 0.8043 - actual_rmse_loss: 4.6387 - val_loss: 0.7631 - val_root_mean_squared_error: 0.7621 - val_actual_rmse_loss: 4.3954 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8005 - root_mean_squared_error: 0.8004 - actual_rmse_loss: 4.6162\n",
      "Epoch 00003: val_loss did not improve from 0.76313\n",
      "365/365 [==============================] - 264s 724ms/step - loss: 0.8005 - root_mean_squared_error: 0.8004 - actual_rmse_loss: 4.6162 - val_loss: 0.7632 - val_root_mean_squared_error: 0.7622 - val_actual_rmse_loss: 4.3959 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7972 - root_mean_squared_error: 0.7971 - actual_rmse_loss: 4.5973\n",
      "Epoch 00004: val_loss improved from 0.76313 to 0.75828, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 264s 724ms/step - loss: 0.7972 - root_mean_squared_error: 0.7971 - actual_rmse_loss: 4.5973 - val_loss: 0.7583 - val_root_mean_squared_error: 0.7573 - val_actual_rmse_loss: 4.3675 - lr: 3.0000e-04\n",
      "[0, 0, 0, 1, 0]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8312 - root_mean_squared_error: 0.8311 - actual_rmse_loss: 4.7933\n",
      "Epoch 00001: val_loss improved from inf to 0.78263, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 269s 723ms/step - loss: 0.8312 - root_mean_squared_error: 0.8311 - actual_rmse_loss: 4.7933 - val_loss: 0.7826 - val_root_mean_squared_error: 0.7815 - val_actual_rmse_loss: 4.5076 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8299 - root_mean_squared_error: 0.8298 - actual_rmse_loss: 4.7859\n",
      "Epoch 00002: val_loss improved from 0.78263 to 0.78220, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 265s 726ms/step - loss: 0.8299 - root_mean_squared_error: 0.8298 - actual_rmse_loss: 4.7859 - val_loss: 0.7822 - val_root_mean_squared_error: 0.7811 - val_actual_rmse_loss: 4.5051 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8278 - root_mean_squared_error: 0.8277 - actual_rmse_loss: 4.7736\n",
      "Epoch 00003: val_loss improved from 0.78220 to 0.77917, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 264s 723ms/step - loss: 0.8278 - root_mean_squared_error: 0.8277 - actual_rmse_loss: 4.7736 - val_loss: 0.7792 - val_root_mean_squared_error: 0.7781 - val_actual_rmse_loss: 4.4876 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8255 - root_mean_squared_error: 0.8254 - actual_rmse_loss: 4.7603\n",
      "Epoch 00004: val_loss improved from 0.77917 to 0.77711, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 264s 724ms/step - loss: 0.8255 - root_mean_squared_error: 0.8254 - actual_rmse_loss: 4.7603 - val_loss: 0.7771 - val_root_mean_squared_error: 0.7760 - val_actual_rmse_loss: 4.4759 - lr: 3.0000e-04\n",
      "[0, 0, 0, 0, 1]\n",
      "5.767554285835394\n",
      "Epoch 1/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.8015 - root_mean_squared_error: 0.8014 - actual_rmse_loss: 4.6219\n",
      "Epoch 00001: val_loss improved from inf to 0.76600, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 270s 724ms/step - loss: 0.8015 - root_mean_squared_error: 0.8014 - actual_rmse_loss: 4.6219 - val_loss: 0.7660 - val_root_mean_squared_error: 0.7650 - val_actual_rmse_loss: 4.4119 - lr: 3.0000e-04\n",
      "Epoch 2/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7791 - root_mean_squared_error: 0.7790 - actual_rmse_loss: 4.4929\n",
      "Epoch 00002: val_loss did not improve from 0.76600\n",
      "365/365 [==============================] - 265s 725ms/step - loss: 0.7791 - root_mean_squared_error: 0.7790 - actual_rmse_loss: 4.4929 - val_loss: 0.7762 - val_root_mean_squared_error: 0.7753 - val_actual_rmse_loss: 4.4716 - lr: 3.0000e-04\n",
      "Epoch 3/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7692 - root_mean_squared_error: 0.7690 - actual_rmse_loss: 4.4355\n",
      "Epoch 00003: val_loss improved from 0.76600 to 0.75985, saving model to convlstm_weights_pr_modified.h5\n",
      "365/365 [==============================] - 265s 726ms/step - loss: 0.7692 - root_mean_squared_error: 0.7690 - actual_rmse_loss: 4.4355 - val_loss: 0.7598 - val_root_mean_squared_error: 0.7588 - val_actual_rmse_loss: 4.3766 - lr: 3.0000e-04\n",
      "Epoch 4/4\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.7619 - root_mean_squared_error: 0.7618 - actual_rmse_loss: 4.3936\n",
      "Epoch 00004: val_loss did not improve from 0.75985\n",
      "365/365 [==============================] - 265s 725ms/step - loss: 0.7619 - root_mean_squared_error: 0.7618 - actual_rmse_loss: 4.3936 - val_loss: 0.7674 - val_root_mean_squared_error: 0.7664 - val_actual_rmse_loss: 4.4202 - lr: 3.0000e-04\n"
     ]
    }
   ],
   "source": [
    "for mask in itertools.islice(mask_list, 11, 15, 1):\n",
    "    print(mask)\n",
    "    X, Y = load_dataset(mask = mask, end_index = end_index)\n",
    "    X, Y, std_observed = set_data(X,Y,end_index=end_index)\n",
    "    print(std_observed)\n",
    "    train_generator, test_generator = data_generator(X, Y)\n",
    "    history = train(\n",
    "        model,\n",
    "        train_generator, \n",
    "        test_generator,\n",
    "        load_weights = True,\n",
    "        std_observed = std_observed\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('Sarth-tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7045c220f384a918686f71b3c00acdc45661fbf73a30bf379cbbfb71cfa47811"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
